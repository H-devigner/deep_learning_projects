{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "  def __init__(self, d, n_heads=2):\n",
    "    super(MyMSA, self).__init__()\n",
    "    self.d = d\n",
    "    self.n_heads = n_heads\n",
    "    \n",
    "    assert d % n_heads == 0, \"The dimension d of the token must be divisible by number of heads n_heads\"\n",
    "    \n",
    "    self.d_head = int(d / n_heads)\n",
    "    \n",
    "    self.q_mappins = [nn.Linear(self.d_head, self.d_head) for _ in range(n_heads)]\n",
    "    self.k_mappins = [nn.Linear(self.d_head, self.d_head) for _ in range(n_heads)]\n",
    "    self.v_mappins = [nn.Linear(self.d_head, self.d_head) for _ in range(n_heads)]\n",
    "    \n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "  \n",
    "  def forward(self, sequences):\n",
    "    result = []\n",
    "    for sequence in sequences:\n",
    "      seq_result = []\n",
    "      for head in range(self.n_heads):\n",
    "        q_mappin = self.q_mappins[head]\n",
    "        k_mappin = self.k_mappins[head]\n",
    "        v_mappin = self.v_mappins[head]\n",
    "        \n",
    "        seq = sequence[:, head * self.d_head:(head + 1) * self.d_head]\n",
    "        \n",
    "        q, k, v = q_mappin(seq), k_mappin(seq), v_mappin(seq)\n",
    "        \n",
    "        attention = self.softmax((q @ k.T) / np.sqrt(self.d_head))\n",
    "        \n",
    "        seq_result.append(attention @ v)\n",
    "      result.append(torch.hstack(seq_result))\n",
    "    return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVIT(nn.Module):\n",
    "  def __init__(self, input_shape, n_patches=7, hidden_d=8, n_heads=2):\n",
    "    super(MyVIT, self).__init__()\n",
    "    self.input_shape = input_shape # (C, H, W)\n",
    "    self.n_patches = n_patches\n",
    "    self.patch_size = (input_shape[1] / n_patches, input_shape[2] / n_patches)\n",
    "    self.input_d = int(input_shape[0] * self.patch_size[0] * self.patch_size[1])\n",
    "    \n",
    "    self.hidden_d = hidden_d # even if C > 1 we will keep this dimension and we pass each row vector of 16 to the linear mapper each time\n",
    "    \n",
    "    assert input_shape[1] % n_patches == 0, \"Height mst be divisible by number of patches\"\n",
    "    assert input_shape[2] % n_patches == 0, \"Width mst be divisible by number of patches\"\n",
    "    \n",
    "    # 1) linear mapper\n",
    "    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "    \n",
    "    # 2) classification token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "    \n",
    "    # 3) positional embeddings\n",
    "    # in forward pass\n",
    "  \n",
    "    # 4)a) layer normalization 1\n",
    "    self.ln1 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden_d)) \n",
    "    \n",
    "    # 4)b) multi-head self attention (msa) and classification token\n",
    "    self.msa = MyMSA(self.hidden_d, n_heads)\n",
    "    \n",
    "  def forward(self, images):\n",
    "    n, c, h, w = images.shape\n",
    "    # reshapes the images into patches\n",
    "    patches = images.reshape(n, self.n_patches ** 2, self.input_d)\n",
    "    \n",
    "    # running the patches into the linear mapper for tokenization\n",
    "    tokens = self.linear_mapper(patches) # or embeddings\n",
    "    \n",
    "    # adding the classification token\n",
    "    patches = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "    \n",
    "    # adding the positional embeddings to the tokens\n",
    "    tokens += self.get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d).repeat(n, 1, 1)\n",
    "    \n",
    "    # TRANSFORMER ENCODER BEGINS #############################\n",
    "    # NOTICE: MULTIPLE ENCODER BLOCKS CAN BE STACKED TOGETHER #####\n",
    "    \n",
    "    # running Layer Normalization, MSA and residual connection\n",
    "    out = tokens + self.msa(self.ln1(tokens))\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def get_positional_embeddings(self, sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "      for j in range(d):\n",
    "        result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n",
    "  \n",
    "  def __call__(self, images):\n",
    "    return self.forward(images)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (49) must match the size of tensor b (50) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m MyVIT(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m))\n\u001b[0;32m      4\u001b[0m images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(model\u001b[38;5;241m.\u001b[39mget_positional_embeddings(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m300\u001b[39m), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhot\u001b[39m\u001b[38;5;124m'\u001b[39m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[1;32mIn[65], line 59\u001b[0m, in \u001b[0;36mMyVIT.__call__\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[1;32m---> 59\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[65], line 41\u001b[0m, in \u001b[0;36mMyVIT.forward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     38\u001b[0m patches \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mvstack((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_token, tokens[i])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens))])\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# adding the positional embeddings to the tokens\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_positional_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_patches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_d\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# TRANSFORMER ENCODER BEGINS #############################\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# NOTICE: MULTIPLE ENCODER BLOCKS CAN BE STACKED TOGETHER #####\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# running Layer Normalization, MSA and residual connection\u001b[39;00m\n\u001b[0;32m     47\u001b[0m out \u001b[38;5;241m=\u001b[39m tokens \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsa(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(tokens))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (49) must match the size of tensor b (50) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# main program for testing purposes\n",
    "if __name__ == '__main__':\n",
    "  model = MyVIT(input_shape=(1, 28, 28))\n",
    "  images = torch.rand(3, 1, 28, 28)\n",
    "  print(model(images).shape)\n",
    "  plt.imshow(model.get_positional_embeddings(100, 300), cmap='hot', interpolation='nearest')\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312_venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
